<rss version="2.0">
<channel>
<title>Brandon Rohrer</title>
<link>https://www.brandonrohrer.com</link>
<description>Brandon Rohrer's blog</description>

  <item>
    <title>
    Use Postgres with Python
    </title>
    <link>
    https://brandonrohrer.com/postgres_intro.html
    </link>
    <pubDate>
    Tue, 23 Dec 2025 08:36:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/postgres_intro.html
    </guid>
    <description><![CDATA[

<p>
I’ve been intimidated by Postgres for a long time, but now that
I’ve gone and got it working I feel kind of silly.
It is a wildly powerful tool, so I expected to have a painful installation
with building from source and modifying environment variables, 
but it turns out Postgres is built to be put to use quickly.
</p>

<p>
On Linux, installation is literally one line. Here’s what it looks like on
Ubuntu 24.04, and <a href="https://www.postgresql.org/download/linux/">other distros</a>
are similar.
</p>

<p>
<pre>
sudo apt-get install postgresql
</pre>
</p>

<p>
For MacOS I installed <a href="https://postgresapp.com/downloads.html">Postgres.app</a>.
After a download and a double click, your Postgres server is up and running.
</p>

<p>
Sorry Windows, I haven't tried you yet, but
<a href="https://www.postgresql.org/download/windows/">here's where I'd start</a>.
</p>

<p>
For working with Postgres from Python, the
<a href="https://www.psycopg.org/psycopg3/docs/basic/index.html">psycopg package</a>
is the ticket.
</p>

<p>
Install with uv
</p>

<p>
<pre>
uv add "psycopg[binary]"
</pre>
</p>

<p>
or pip.
</p>

<p>
<pre>
pip install "psycopg[binary]"
</pre>
</p>

<p>
Then the Python interface looks like this.
</p>

<p>
<pre>
import psycopg <br>
with psycopg.connect(dbname="postgres", user="postgres") as conn:
    with conn.cursor() as cursor:
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS
            pets ( name TEXT, weight_kg REAL, birthday DATE);
        """) <br>
        cursor.execute("""
            INSERT INTO pets (name, weight_kg, birthday)
            VALUES (%s, %s, %s)
        """, ('Mugsy', 24.3, '2022-10-18'))<br>
        cursor.execute("SELECT * FROM pets")
        print(cursor.fetchone()) <br>
        conn.commit()
</pre>
</p>

<p>
It's very much like
<a href="https://brandonrohrer.com/db_first_db.html">working with <code>sqlite3</code></a>,
but with a few small differences.
</p>

<p>
It’s helpful to be aware of the fact that the Postgres server is its
own process. It’s a separate program that you have to call from Python.
The fact that it’s not tied to the Python interpreter and can run
its own optimized code, purpose built for your operating system,
allows it to really scream.
</p>

<p>
The example above is pretty dense, especially if you’ve never worked
with databases before. Here’s a walk-through.
</p>

<p>
<br>
<pre>
import psycopg
</pre>
Heads up: the import name of the package is <code>psycopg</code>, not <code>psycopg3</code> or <code>psychopg</code>.
</p>

<p>
<br>
<pre>
with psycopg.connect(dbname="postgres", user="postgres") as conn:
</pre>
The <code>with</code> construction is a Python context manager, a shorthand for saying
"do some stuff with this database connection and, whether it finishes
successfully or crashes and burns, make sure to gracefully excuse yourself
to the server when you're done and close everything down
with as little damage as possible."
</p>

<p>
The <code>dbname="postgres"</code> is an instruction to connect to the <code>postgres</code>
database, one that exists by default on all Postgres servers.
A database is a collection of tables, the same way a spreadsheet
"workbook" is a collection of individual sheets. Initially
the <code>postgres</code> database will be empty, containing no tables.
</p>

<p>
The <code>user="postgres"</code> says to connect as the user named <code>postgres</code>,
a default user with default permissions.
</p>

<p>
Once the connection is established, return it as <code>conn</code>.
</p>

<p>
<br>
<pre>
    with conn.cursor() as cursor:
</pre>
A connection to a database isn't enough. You also need a cursor.
A cursor will let you create tables, add data, and run queries.
There's more about cursors in this
<a href="db_awkwardness.html">intro to databases post</a>.
</p>

<p>
<br>
<pre>
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS
            pets ( name TEXT, weight_kg REAL, birthday DATE);
        """)
</pre>
Make a new table called <code>pets</code>. If one already exists (for example, if you
run this script more than once) then don't worry about it.
</p>

<p>
Give the table three columns.
</p>

<ul>
<li> The first is called <code>name</code> and expects text data.</li>
<li> The second is called <code>weight_kg</code> and expects real valued
(floating point numerical) data.</li>
<li> The third is called <code>birthday</code> and
expects a date. There are a lot of allowable
<a href="https://www.postgresql.org/docs/current/datatype-datetime.html#DATATYPE-DATETIME-INPUT-DATES">Postgres date formats</a>. </li>
</ul>

<p>
<br>
<pre>
        cursor.execute("""
            INSERT INTO pets (name, weight_kg, birthday)
            VALUES (%s, %s, %s)
        """, ('Mugsy', 24.3, '2022-10-18'))
</pre>
Add a row to this table.
</p>

<ul>
<li> Put <code>Mugsy</code> in the <code>name</code> column.</li>
<li> Put <code>24.3</code> in the <code>weight_kg</code> column.</li>
<li> Put October 18, 2022 in the <code>birthday</code> column.</li>
</ul>

<p>
This format with the <code>%s</code> placeholders is important. This lets Postgres
perform input validation&mdash;to check whether any of the input values
will break things. If you are familiar with Python string construction
it would be tempting to substitute the values directly in with an f-string.
</p>

<p>
<pre>
pet_name = 'Mugsy'
pet_weight = 24.3
pet_birthday = '2022-10-18'
f"""
    INSERT INTO pets (name, weight_kg, birthday)
    VALUES ({pet_name}, {pet_weight}, {pet_birthday})
"""
</pre>
Don't do this! It opens you up to SQL injection attacks. If someone
names their Pomeranian <code>Fluffy'); DROP TABLE pets;--</code>
then badness will happen [ <a href="https://xkcd.com/327">xkcd 327</a> ].
</p>

<p>
<br>
<pre>
        cursor.execute("SELECT * FROM pets")
        print(cursor.fetchone())
</pre>
Run a query, selecting all the rows from your one-row table.
</p>

<p>
<br>
<pre>
        conn.commit()
</pre>
Until you commit it everything you've done is temporary. <code>commit()</code>
makes it permanent.
</p>

<p>
<hr>
</p>

<p>
That's a foothold in the Postgres world. From there, your imagination is the only limit.
If you want to brush up on (or start learning) SQL,
<a href="sql_resources.html">here are some community-recommended resources</a>.
I also recommend bookmarking the
<a href="https://www.postgresql.org/docs/18/index.html">PostgreSQL documentation</a>.
It is exhaustive.
</p>

<p>
The really cool thing about <a href="https://en.wikipedia.org/wiki/PostgreSQL">Postgres</a>
(formally known as PostgreSQL)
is that it is a full-blown,
production ready, run-with-the-big-dogs database. Just ask
<a href="https://en.wikipedia.org/wiki/PostgreSQL#Notable_users">Reddit, Instagram, and OpenAI</a>.
</p>
    ]]></description>
  </item>

  <item>
    <title>
    Moving your code from prototype to production
    </title>
    <link>
    https://brandonrohrer.com/hosting8.html
    </link>
    <pubDate>
    Fri, 12 Dec 2025 06:34:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/hosting8.html
    </guid>
    <description><![CDATA[

<p>
So you wrote a cool little weather app that predicts temperatures
in your neighborhood.
Now you want to be able to share those predictions with everyone of
the thousand people who live there. How do you go from
"It works on my machine" to "No matter where you are in the world you can
use this in your own browser."?
</p>

<p>
There are a lot of dimensions to making your code ready to leave the nest
and be on its own in the real world, but an important one is
deploying it from your laptop to a globally available server.
</p>

<p>
This post is about one good way to do this. There are lots of others,
but this one is solid and, believe it or not, one of the simplest.
This deployment starter kit includes
</p>

<ul>
<li> <a href="https://flask.palletsprojects.com/en/stable/">Flask</a>, a Python package for turning your code into an app.</li>
<li> <a href="https://gunicorn.org">Gunicorn</a>, a more serious production grade WSGI server.</li>
<li> <a href="https://nginx.org/en/">nginx</a>, an industry standard web server.</li>
<li> <a href="https://wiki.ubuntu.com/UncomplicatedFirewall">ufw</a>, an Uncomplicated Firewall for Linux</li>
<li> <a href="https://systemd.io">systemd</a> the Linux tool for turning your code into a daemon.</li>
</ul>

<p>
I won’t bother walking through How to do this because DigitalOcean has
<a href="https://flask.palletsprojects.com/en/stable/deploying/gunicorn/">an amazing documentation page</a>
for setting this up in Ubuntu 22.04. It worked for me on 24.04 as well.
Follow their page to the letter and it will get you safely through.
And it is such a cool feeling when it does.
</p>

<p>
However, I feel like it is worth spending a couple minutes on the Why.
Why do any of this? Why not just share your code from your personal computer?
You could if you wanted to. But it turns out that
the Internet is an extremely busy and occasionally harsh place to be.
Prototyping on your laptop is like setting up a lemonade stand in a private garden
and selling cups to your mom. Opening your service up to the Internet
unfiltered is moving your card table lemonade stand to an active combat zone.
</p>

<p>
Luckily, there are really helpful tools for handling this.
</p>

<p>
<strong>Flask</strong> is the first layer of order we add.
It takes willy-nilly functions and makes sure they fit the structure of an API.
They turn your Python into a web app.
Flask takes your lemonade stand, and puts four walls, a ceiling, and
a well defined front door on it.
</p>

<p>
<strong>Gunicorn</strong> (short for Green Unicorn) is an open source WSGI server written
in Python. WSGI is short fo
<a href="https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface">web server gateway interface</a>,
and happily can be pronounced "whiskey". It serves as a bridge between a web app
and a web server. Gunicorn handles scalability and adds some security.
Gunicorn beefs up the walls of your lemonade stand and adds a headset-wearing
coordinator that lines up all the customers and makes sure they keep
moving, get their lemonade efficiently, and get out. It also builds
carbon-copies of your lemonade stands next to the original and forms a queue
of customers in front of each one. Gunicorn gets you ready to handle
the throes of people that want to come visit your stand.
</p>

<p>
<strong>nginx</strong> is a fully featured HTTP server. It often gets used on its own
to serve webpages, but it can also route API requests. In our setup,
we are going to use it as a
<a href="https://en.wikipedia.org/wiki/Reverse_proxy">reverse proxy</a>,
an extra front door and lobby with a security guard and a metal detector
that people have to get through before they can line up for lemonade.
nginx turns away lot of troublemakers who might be trying to sneak in
and take our lemons or trick our customers out of their cash.
nginx helps make sure that the people who show up are there to buy lemonade.
</p>

<p>
<strong>ufw</strong> provides additional security. It has one job, which is to check
people’s ID and destination (IP address and port).
You can feed it a list of known bad actors' IPs to block them preemptively.
ufw is a ridiculously high compound wall that surrounds your lemonade complex,
providing a comfortable layer of protection from anyone seeking
to do violence or so mayhem.
</p>

<p>
<strong>systemd</strong> lets us run Gunicorn as a daemon, a background process that
is automatically started when you turn the computer on and restarted
when necessary. Once you set up Gunicorn as a daemon, you don’t have
to worry about whether your lemonade vendors fell asleep, or decided
to go home. Nope, with systemd the lemonade emporium is up and running 24/7.
</p>

<p>
When you consider all of the things that could go wrong with a service
exposed to the Internet, it’s kind of amazing that we can get it to work at all,
which makes this setup extremely cool. Granted, it has more moving pieces
than I would prefer, but they all serve a purpose. Importantly,
the instructions for setting them up are well defined. There are other,
fancier ways to do all of these steps, and ways to handle yet more security
and yet larger scale, but this Flask/Gunicorn/nginx set up is something
you can put out into the world without apology.
It’s a fully-baked grown-up solution.
</p>


    ]]></description>

  </item>
  <item>
    <title>
    Bomb-proof your website with mirroring
    </title>
    <link>
    https://brandonrohrer.com/hosting7.html
    </link>
    <pubDate>
    Sat, 29 Nov 2025 08:36:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/hosting7.html
    </guid>
    <description><![CDATA[

<p>
In previous posts, we’ve done our best to set up a website that is resilient
to traffic, spikes, bots, and mischief of all sorts. (
<a href="hosting.html">I</a>,
<a href="hosting2.html">II</a>,
<a href="hosting3.html">III</a>,
<a href="hosting4.html">IV</a>,
<a href="hosting5.html">V</a>
<a href="hosting6.html">VI</a>
<a href="hosting7.html">VII</a>
)
But there are still plenty of other things that can go wrong.
A good way to protect against this is mirroring.
</p>

<p>
A mirror is like a backup website&mdash;not just a backup with the data
but of everything, including the server and domain name.
</p>

<p>
I built two additional mirrors for my blog website <code>brandonrohrer.com</code>.
The symmetry of
<a href="https://en.wikipedia.org/wiki/Triple_modular_redundancy">triple mode redundancy</a>
appeals to me, especially after hearing that it’s
<a href="https://llis.nasa.gov/lesson/18803">a principal used by NASA</a>
and growing up on
<a href="https://en.wikipedia.org/wiki/Rendezvous_with_Rama">some triples-themed sci-fi</a>.
You can take down one, but you still have two backups to work with,
plenty of breathing room. If you get really unlucky,
two will go down leaving you with a third.
Why build one when you can build three at three times the cost?
</p>

<p>
I tried to build in as much redundancy as possible.
I got three domain names, each from a different domain name registrar.
</p>

<ul>
<li> <a href="https://brandonrohrer.com">brandonrohrer.com</a>
from <a href="https://www.namecheap.com">Namecheap</a>, based in Phoenix.</li>
<li> <a href="https://brandonrohrer.org">brandonrohrer.org</a>
from <a href="https://www.hover.com">Hover</a>, based in Toronto.</li>
<li> <a href="https://brandonrohrer.at">brandonrohrer.at</a> from
<a href="https://www.hostpoint.ch/en/domains/domains.html">Hostpoint</a>, based outside Zurich.</li>
</ul>

<p>
These are running on three different virtual private servers (VPSs).
</p>

<ul>
<li> A VPS located in New York from <a href="https://www.digitalocean.com">DigitalOcean</a>,
which is headquartered near Denver.</li>
<li> A VPS located in Prague from <a href="https://www.ovhcloud.com/en/">OVHCloud</a>,
which is headquartered in France on the border with Belgium.</li>
<li> A VPS located in Montreal, from <a href="https://www.koumbit.org/en">Koumbit</a>,
which is also headquartered in Montreal.</li>
</ul>

<p>
They all contain the same content, which lives in a repository called
<code>blog-website</code>. For additional redundancy, I host this repo on three
different git services.
</p>

<ul>
<li> <a href="https://github.com">GitHub</a>, a Microsoft property.</li>
<li> <a href="https://gitlab.com">GitLab</a>, operated out of Utrecht.</li>
<li> <a href="https://codeberg.org">Codeberg</a>, a non-profit in Berlin.</li>
</ul>

<p>
One benefit from having three mirrors is that I can treat one as a
staging environment. If I want to make a risky change, such as automatically
generating new firewall rules to block annoying traffic, I can do it on
my least traveled mirror and see how it goes. If something goes horribly
wrong and takes down that entire server, then I can put it back together
from scratch, all while the other two mirrors stay operational.
</p>

<p>
Manually updating three mirrors is a little tedious. It’s easy to see
why websites with mirrors or other content distribution networks would
automate this. However, several of the larger corporate outages we’ve seen
recently are precisely because of these automated deployment mechanisms.
Because there’s one system that impacts every website, a single
misconfiguration can take down all of them. They aren't isolated from
each other. I’m not running a business that needs instant updates, so
I can afford to roll mine out slowly, by hand, and take that extra time
and get that extra resilience.
</p>

<p>
There are two parts of my system that are single points of failure.
The first is my content hosting service. All of the images and videos
associated with my blog are hosted in a separate repository on GitHub.
This keeps the bandwidth to my VPSs low and keeps my load times very snappy,
minus the images. Then I let GitHub pay the bandwidth costs for the larger
files and take advantage of its global network. This comes at the cost of
some fragility. If for some reason my GitHub account should be compromised,
I would have to find another content hosting service. (I of course have
multiple backups of all the files there and could rebuild.) But for my purposes,
it’s good enough for now.
</p>

<p>
The other single point of failure is me. If I were running a serious
operation, there would be at least three administrators with full admin
permissions on everything so that if one or two of us were incapacitated,
the third could still carry on. In this particular instance, I’m not worried.
If I’m in a position where I’m not capable of taking care of the blog,
then likely the last thing I'll be worried about is the health of the blog.
</p>

<p>
Spreading services across companies and across continents is a good
way to prevent a single event from taking down your site. It's not a good
feeling to be dependent on a single corporation or even national government
for your internet real estate. If AWS us-east-1 goes down and takes your site
with it, that's a sad day. It's nice to know that your little piece of
the web will stay standing through anything short of a global calamity.
</p>

    ]]></description>

  </item>

  <item>
    <title>
    Automating traffic control
    </title>
    <link>
    https://brandonrohrer.com/hosting6.html
    </link>
    <pubDate>
    Wed, 12 Nov 2025 06:34:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/hosting6.html
    </guid>
    <description><![CDATA[


<p>
In <a href="hosting5.html">my last post</a> I said "Automating firewall updates will be
an adventure for another day." Today is that day. This is the latest in a
growing series on hosting your own static blog. (
<a href="hosting.html">I</a>,
<a href="hosting2.html">II</a>,
<a href="hosting3.html">III</a>,
<a href="hosting4.html">IV</a>,
<a href="hosting5.html">V</a>
<a href="hosting6.html">VI</a>
<a href="hosting7.html">VII</a>
)
</p>

<p>
Protecting your website requires you to keep your eyes on it at all times.
That can get tiring. Sometimes you need to eat and sleep and maybe work
for a living. It can be useful to have automated helpers to fill those gaps.
</p>

<h2><a id="Detection"></a><a href="#Detection">Detection</a></h2>

<p>
The first step in keeping your blog traffic healthy is to be able to detect
when someone is doing something they shouldn't. In my
<a href="hosting5.html">traffic control post</a> I described five behaviors that I
wanted to limit
</p>

<ol>
<li> Scanning for secrets</li>
<li> Fishing for files</li>
<li> Rapid-fire requests</li>
<li> Trying to hide rapid-fire requests</li>
<li> Attempting unsupported actions</li>
</ol>

<p>
The post describes these in words, but they need to be made more specific
before they can be enforced by an automated detector. Doing this step well
is where most of the art and cleverness is required. If you define a rule
too narrowly, there will be a lot of IP addresses that <em>should</em> get blocked,
but get missed (false negatives for the detector). On the other hand, if
you define a rule too broadly, there will be a lot of legitimate IP traffic
that gets blocked (false positives) and you will have a lot of sad readers
who can't access your cat pictures.
</p>

<p>
There is also value in having rules that are simple as possible. The more
caveats and quirks in a ruleset, the harder it is to mentally keep track
of what combination of conditions will pass and what will be blocked. In
exchange for more flexibility in defining which behaviors get blocked, you
have a higher cognitive burden for keeping track of it and closing loopholes
and corner cases that an enterprising asshole might exploit.
</p>

<p>
With all these things in mind, here is my attempt at a rule set to catch the
bad behaviors I listed. They're based on the URLs/pages an IP address
tries to access, the http actions they use, and the http status codes they
generate.
</p>

<p>
There are some behaviors that are clearly mischievous.
Doing these things even once show that you are up to no good and get you a block.
These are <strong>one-strike-and-you're-out</strong> behaviors.
</p>

<p>
Other behviors are only a problem if they are repeated. Trying to access
a page that's not there and generating a 404 status happens all the time
in normal browsing. Mis-typed URLs and outdated links are part of browsing
life. But a whole lot of 404s from the same URL starts to look like someone
is fishing. These are <strong>n-strikes-and-you're-out</strong> behaviors.
</p>

<p>
Walking through my logs helped me learn which pages, statuses, and actions
are most likely to occur for my server. Here is my list as of this writing,
although it grows over time. The full current list is always available
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/config.py">here</a>.
</p>

<h3><a id="One-strike-pages"></a><a href="#One-strike-pages">One-strike pages</a></h3>

<p>
Right now it's just <code>.env</code>. There are other targets that are problematic,
like <code>.git/config</code> or <code>password</code>, but so far every IP that has tried
to hit one of those has also tried to hit <code>.env</code>. Because this is a very
strict rule I want to make sure to limit it to certain violations. I don't
want to lock out anyone making an honest mistake. But if you're going after
a <code>.env</code> file you're clearly looking for someone else's secrets.
</p>

<p>
Another way to keep this narrow is that the match needs to be exact.
<code>.environment</code> or <code>.enviable</code> would not get an IP address blocked, only URLs
like <code>config/.env</code> or <code>.env/tokens</code> or even just <code>.env</code>.
</p>

<h3><a id="n-strike-pages"></a><a href="#n-strike-pages">n-strike pages</a></h3>

<p>
A big difference with n-strike pages is that they only need to be
partial matches on the filename. Most of them are filename extensions.
This allows them to cast a wider net, and because they have to be repeated
n times, there is still not a big risk that they will occur accidentally during
innocent browsing.
</p>

<p>
The n-strike pages are problematic, but could more plausibly be accidental.
Some are compressed archives, like <code>.7z</code>, <code>.gz</code>, <code>.rar</code>, or <code>.zip</code>. I'm not
currently hosting any of these on my site, so anyone trying to hit one
repeatedly is hunting. Others are for PHP content, <code>.php</code>, or for
WordPress content, <code>wp-includes</code> and <code>wp-content</code>. I don't have any of these
either, but crawlers love to scan for common WordPress and PHP filenames
anyway, especially those associated with site administration.
I went with <em>n</em> = 5 on these rules.
</p>

<p>
This is a good example of how my one-strike and n-strike rules are specific
to my site. There's nothing wrong with hosting WordPress files, I just
happen not to be doing it, so I can use it as a bad behavior signal. What these
rules lack in sophistication they make up for in the ability to be tailored
to your site content and use cases.
</p>

<h3><a id="One-strike-actions"></a><a href="#One-strike-actions">One-strike actions</a></h3>

<p>
There are
<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Methods">HTTP actions</a>
that don't occur often in casual browsing and can
be red flags.
</p>

<p>
<code>PROPFIND</code> tries to retrieve a table of contents list of all the files on
the server. It's not used during normal browsing, and smacks of scraping and
hunting for vulnerabilities. Blocked.
</p>

<p>
<code>CONNECT</code> tries to set up a tunnel to a remote server. It is
used for more sophisticated networking than I'm doing, and having it show up
where it shouldn't makes me nervous. Blocked.
</p>

<p>
<code>SSTP_DUPLEX_POST</code> is also about setting up connections, specifically
encrypted connections like for ssh and VPNs. And there is absolutely no
reason someone should be doing it with my server. Blocked.
</p>

<h3><a id="n-strike-actions"></a><a href="#n-strike-actions">n-strike actions</a></h3>

<p>
There are other more benign actions that my server doesn't support. One
or two of these is not an issue but a large number of them suggests that
someone is up to no good. (I used <em>n</em> = 8.)
</p>

<p>
<code>POST</code> is very common and useful for any data
transfer operations from the client to the server, say when a customer
attempts to log in or fills out a form. It's just that I don't host any
activities that require a POST so if someone is making dozens of POST
requests I comfortably cut them off.
</p>

<p>
Occasionally empty HTTP requests come through - no GET, no POST, no nothing.
These are just weird and that make me suspicious, so when they come in groups
they are a blockable offense.
</p>

<h3><a id="n-strike-statuses"></a><a href="#n-strike-statuses">n-strike statuses</a></h3>

<p>
I haven't found any statuses yet that I need to block on the first occurrence,
but there are several that are probably indicators of funny business.
For these I settled on <em>n</em> = 8.
</p>

<p>
<code>403 Forbidden</code> is the code for when the requested page is off limits.
If this happens once or twice it might be an accident, but if it happens a lot
someone is probably trying to get into something they shouldn't.
</p>

<p>
<code>404 Not Found</code> A couple not-found files is to be expected, but lots and lots
of them means means someone is searching for something they haven't been
invited to see, like looking through your mom's closet for your Christmas
presents.
</p>

<p>
<code>429 Too Many Requests</code> Someone can be click-happy occasionally and generate
a 429, but a string of them means than an IP address is hammering your site.
That's just rude, and in extreme cases it can make it hard for other
people to get their page requests in.
</p>

<h2><a id="Blocking"></a><a href="#Blocking">Blocking</a></h2>

<p>
The checks above are implemented in
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/autoblock.py">autoblock.py</a>.
It walks through these rules for the current day's access logs and compiles
a list of IP addresses that have earned a block.
</p>

<p>
<code>autoblock.py</code> also takes that list of IP addresses and uses them to create
a shell script which, when run as root, blocks those addresses
in the <code>ufw</code> (Uncomplicated FireWall).
</p>

<p>
To fully automate this, both <code>autoblock.py</code> and the shell script are added
to cron jobs.
</p>

<p>
For the cron jobs of the not-special user <code>brohrer</code>:
</p>

<p>
<pre>
\<em>/5 \</em> \<em> \</em> \*
/home/brohrer/.local/bin/uv run
--project /home/brohrer/webserver-toolbox/
/home/brohrer/webserver-toolbox/autoblock.py >>
/home/brohrer/webserver-toolbox/logs/cron.log 2>&1
</pre>
</p>

<ul>
<li> Every five minutes</li>
<li> run with <code>uv</code></li>
<li> the project <code>webserver-toolbox</code></li>
<li> and the script <code>autoblock.py</code> within</li>
<li> and dump all of the errors (stderr) and outputs (stdout)
to the file <code>cron.log</code>.</li>
</ul>

<p>
And for root's cron jobs:
</p>

<p>
<pre>
3-59/5 \<em> \</em> \<em> \</em>
sh /home/brohrer/webserver-toolbox/update_firewall.sh >>
/home/brohrer/webserver-toolbox/logs/cron.su.log 2>&1
</pre>
</p>

<ul>
<li> Every five minutes, starting from minute 3 of the hour</li>
<li> run the shell script <code>update_firewall.sh</code></li>
<li> and dump stderr and stdout to the log file <code>cron.su.log</code> .</li>
</ul>

<p>
The decision to run this every five minutes was a trade-off between wanting to
shut down misbehavior quickly and not wanting to wantonly burden the server.
As you've probably noticed there are lots of small decisions that have gone
into setting this up. Some of them are hard coded into the cron jobs,
like the timing of the automated runs and the names of the log files,
but the rest is concentrated in
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/config.py"><code>config.py</code></a>.
If you want to pull down the
<a href="https://codeberg.org/brohrer/webserver-toolbox">webserver-toolbox</a> and
use it yourself, you can do a lot of customization there.
</p>

<p>
There are lots of different reasons a particular website might have to
block an IP address, and they will be very different depending on the
website and the expected access patterns. For instance, one
of the most common IP-blocking tools,
<a href="https://en.wikipedia.org/wiki/Fail2ban">Fail2ban</a>, focuses a lot on
failed login attempts. Too many and too fast is grounds for blocking an
IP. That makes a lot of sense for any website with
a login, but no sense for my website which is just static html with no
access restrictions. The lesson here isn't that one way is best, but
that the best way for one website will not be the same as another.
And a custom blocker, tailored to the function and usage of a particular
websiste, will always be more effective than a generic, one-size-fits-all
tool.
</p>

<h2><a id="Automation"></a><a href="#Automation">Automation</a></h2>

<p>
Coming back to the project at hand, we now have an automated blocker
that is happily spinning away, catching
violators every five minutes and banishing them from the joys of the
website. Sadly, we've only gotten started.
</p>

<p>
In the world of robotics, an automated system is often described as
a sense-decide-act loop. In the case if IP blocking, "sense" is
recording the access logs. <code>nginx</code> takes care of that. The "decide"
portion is what happens in <code>autoblock.py</code>, where violators are detected.
And the "act" portion is the actual blocking of the IPs in the firewall.
</p>

<p>
Describing this as a loop reminds us that it's a cycle that continues
to run over time. The earth will spin, people will go to bed and wake up,
trees will grow, and pendulums will swing. Some of these events affect our
system. When a trans-oceanic telecommunications cable gets cut, when
a blog post goes viral, when some dork wakes up and thinks of a new way to take down
my website, those events interfere with the predictable patterns of the
carefully tuned sense-decide-act loop that was created before those things
happened.
</p>

<p>
Getting an automation to run once, when you are paying close attention to it,
means that you have a nice demo. It doesn't mean tha you have a system that
is robust, that can weather the storms of traffic
that increasingly obnoxious webscrapers may throw at it.
For that you need to allow for logging, monitoring, alerts, and
the ability to make changes to the system within guardrails, so that
I don't do anything too drastic like erasing the entire blocklist.
</p>

<h2><a id="Building-with-guardrails"></a><a href="#Building-with-guardrails">Building with guardrails</a></h2>

<p>
When building automated systems with machinery or power electronics,
a bug during development can mean fire, mayhem, and dismemberment,
rather than just an error on the console. If you want to have a long
career as a nerd, it's important to build automated systems with guardrails.
</p>

<p>
One really fun thing you can do when you are administering your own
webserver is to accidentally put your own IP address on the blocklist.
Once that happens, it can only be remedied with truly drastic measures.
In order to prevent this, the autoblock tool starts by adding any
IP addresses in the allowlist to the top of the ufw rule set. Because
ufw goes through the rules sequentially and honors the first applicable rule
it finds, this ensures that any allowlisted addresses will never be blocked.
The autoblocker also takes pains to insert any block rules after the end
of the allow list, so it doesn't supersede any of them.
</p>

<p>
And in the darkest of universes, where something goes horribly wrong and I lock
myself out of my server entirely, I can always rebuild because I've chosen
to keep a copy of all the software tools in
<a href="https://codeberg.org/brohrer/webserver-toolbox">a git repository</a>
outside of the server.
</p>

<p>
Another guard rail is the ability to do a dry run&mdash;to identify all the
addresses that the autoblocker would like to block, but to not act on them
just yet, and to hold off writing the shell script that would add them
to the ufw rule list. This is done with the <code>--dryrun</code> option on
<code>autoblock.py</code>. It is really helpful for verifying that any clever new
blocking rules work as expected.
</p>

<p>
And as an additional convenience there is a <code>--local</code> option so I can do
some quick development on my home machine without having to upload everything
the server first in order to check it. It tightens up the fuck-around-find-out
 so that I can make mistakes faster and with lower stakes and
greater gusto. It uses
a copy of some old log files just to work the wrinkles out of any new
methods that might be under development. I used it a lot when writing
the n-strike checks, and I expect to use it again in the future when
I notice new patterns in IP addresses that I want to use as a block signal.
</p>

<p>
The final guardrail that the autoblocker uses is making a backup copy
of the blocklist each time it runs. That means that I can turn the clock
back if I need to and pretend that any regrettable autoblock runs never
happened.
</p>

<h2><a id="Explainability"></a><a href="#Explainability">Explainability</a></h2>

<p>
Another thing that can happen in an automated system is that someone
might ask why it made a particular decision. The ability to perform an
audit, to look inside the algorithmic thought process of the code,
is especially important when these decisions affect other people.
If someone comes to me asking why they were blocked from my dinky
website, it's good to have an answer and a way to change that decision.
</p>

<p>
Due to the simplicity of the autoblocking rules, it's enough to record
the date and the rule violated by the IP. These are captured in a set
of files in <code>logs/blocks_&lt;rule&gt;.txt</code> of <code>website_tools</code>. If someone comes
to me with a question I can search for their IP address across these
files and find the date and rule that was violated. Then we can have
a conversation and decide
whether to leave them blocked, remove them from the blocklist, or, if
we both expect the violation to recur and are OK with it, add them
to the allowlist.
</p>

<p>
Explainability is often neglected in modern automated systems, especially
if the purpose of those systems is to shield the humans behind them from having
to explain their decisions. Explainability in complex algorithms
like neural networks and large language models is difficult, if not infeasible.
The conclusion I draw from this is that LLMs are not suitable for
automated systems performing any serious task.
</p>

<h2><a id="Monitoring"></a><a href="#Monitoring">Monitoring</a></h2>

<p>
Automating IP blocking doesn't mean that the system won't break. It just
means that I won't be watching it when it does.
In the days before self-driving cars there was a joke about the guy driving
his motor home down the highway and put it on cruise control so he could go
in the back and get a Coke. (It's still funny now but for different reasons.)
Automation doesn't mean we can stop paying attention. It just means we
get to pay attention to different things.
</p>

<p>
It's still important to keep an eye on what's going on, just at a different
level. I have an automated helper that stays up 24/7 watching for rule
breakers and blocking them, but I still need to check in on it.
</p>

<p>
I do this by logging in to the server every couple of days and looking
around. I wrote a couple of helper scripts to do some aggregations.
</p>

<ul>
<li> I can check to total number of access requests to each page with
<code>uv run pages.py</code>. I know from watching that the most popular posts are
on <a href="transformers.html">transformers</a>,
<a href="ssh_at_home.html">setting up ssh</a>,
and <a href="convert_rgb_to_grayscale.html">converting RGB color to grayscale</a>, and
from there they taper off. If I see anything different from this, then I know
something may be off.</li>
<li> I can check the total number of access requests, by IP address with
<code>uv run ips.py</code>. Scrapers and bots tend to have many access requests.
If most of the IP addresses are in the single digits for access requests,
then I'm comfortable
that the overall composition of folks reaching my site is healthy. </li>
<li> I can check which pages were attempted but were not found with
<code>uv run history.py --status 404</code>. This is particularly helpful for
identifying IP addresses that are hunting for sensitive files, rather
than for blog pages.</li>
<li> And finally, I can take a look through the raw logs with
<code>uv run history.py</code>. This is good for using our uniquely human ability
to spot things that just look off, and might merit adding new rules
to the one-strike and n-strike checks.</li>
</ul>

<p>
If I wanted to get more professional about this, I could set up a recurring
script to pull out common cross sections of these data sets and put them
into an attractive plot, but honestly it's not necessary. The most critical
element of good monitoring is spending a few focused minutes looking at
the system from a few different directions until you are convinced it is
doing its job.
</p>

<h2><a id="Alerting"></a><a href="#Alerting">Alerting</a></h2>

<p>
My website is not a critical piece of infrastructure. It's not a
flight control system or a nuclear power plant. If it goes down for
an hour or a day or a week. No one will die. If I'm really lucky,
a few people will notice.
</p>

<p>
That's lucky because sometimes I forget to check on it for a few days.
But if I were really super on top of it, I could implement some alerting
on top of the monitoring. Alerting is, in essence, adding another layer of
automation. I'm automating the part where I hop on the server and look
for unhealthiness in the logs. If I've done my job right in this post so far,
this will be a little bit nervous making. "Wait", you may say, "we just went
to a whole lot of work to make sure that our automation wouldn't break
in horrible ways. Now you're talking about adding <em>another</em> story in this
house of cards?" And you would be correct to ask this.
</p>

<p>
Automating alerts is important for the same reason as automating blocking;
it's usually too much to ask a human to sit there around the clock and
keep and eye on things. And even when constant surveillance is in place,
it may still be too much to ask of a human to watch millions of log entries
per second or to keep track of dozens of gauges simultaneously. Alerts serve
to draw a human observer's attention when they are absent or distracted.
</p>

<p>
Implementing alerting well is every bit as hard as implementing autoblocking
well. It's another sense-decide-act loop, where the "act" part is activating
an alert. Monitoring is the "sense" side of things. Determining what to
monitor is an art in itself, related to feature engineering, which I'll have
to cover in another post.
</p>

<p>
And "decide" is a separate design problem.
After I've found a useful signal to sense, say, the number of access requests
resulting in a 404 Not Found status, determining how many 404's represent
an alert-worthy event is a tricky trade-off. Make the threshold too low,
and the alert becomes too sensitive, resulting in many false positivites&mdash;a
nuisance alarm, like the boy who cried wolf. Even if it's right sometimes,
it's too much work to check it out every time it fires so it gets ignored.
And if the threshold is too high, then the alert isn't sensitive enough.
It misses genuine problems, has a high false negative rate. Incorporating
additional factors like changing how long of a time window to count over
or making allowances for seasonality in the behavior can help balance
false positives and false negatives, but also introduce additional complexity
of their own.
</p>

<p>
Alerts a very useful, but they also deserve careful design attention,
testing, and periodic reevalution. And then of course there is monitoring
for your alerting system, and health checks and automated alerts in case
your alerts go down. It's just turtles all the way down, as deep as you
care to take it.
</p>

<p>
But if all you're building is a derpy blog website, then you don't need to
worry about them.
</p>

<p>
November 11, 2025
</p>

    ]]></description>

  </item>

  <item>
    <title>
    (Web)Traffic Control 
    </title>
    <link>
    https://brandonrohrer.com/hosting5.html
    </link>
    <pubDate>
    Wed, 08 Oct 2025 08:36:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/hosting5.html
    </guid>
    <description><![CDATA[


<p>
The internet is a pretty rough and tumble place to hang out.
It's not even that there are so many jerks out there, it's just that
all of them can reach you at once.
Luckily you have the power to shut them down.
</p>

<p>
This installment focuses on blocking IP addresses that are doing things
they shouldn't. It's the lastest in a series of posts on
</p>

<ol>
<li> <a href="hosting.html">Setting up a webserver</a></li>
<li> <a href="hosting2.html">Setting up a domain name</a></li>
<li> <a href="hosting3.html">Setting up some security</a></li>
<li> <a href="hosting4.html">Keeping a webserver healthy</a></li>
</ol>

<h2><a id="Block-an-IP-address"></a><a href="#Block-an-IP-address">Block an IP address</a></h2>

<p>
The most straightforward way to block and IP address is in the firewall.
It is the tool build specifically for this.
</p>

<p>
To block the address <code>101.101.101.101</code>, run from the command line
</p>

<p>
<pre>
sudo ufw insert 1 deny from 101.101.101.101
</pre>
</p>

<p>
This instructs ufw (the Uncomplicated FireWall) to insert a rule at the
the top of the list (position 1) to deny all incoming traffic from
the address. After running this, no restart of the firewall is needed.
The rule is active.
(<a href="https://www.digitalocean.com/community/tutorials/ufw-essentials-common-firewall-rules-and-commands)">ufw docs</a>
</p>

<p>
The position 1 is important because in ufw, the first rule that matches
is applied. If there was a rule to allow all addresses that started
with <code>101.</code> and that rule came before the deny rule, then the deny
rule would never be reached.
</p>

<p>
While it's possible to block specific ports, or even to block an
IP address from seeing particular pages, complex rules
and conditions get difficult to analyze very quickly, and can lead to cases
where there are loopholes. Use fancy rule combinations sparingly.
</p>

<h2><a id="Parse-logs"></a><a href="#Parse-logs">Parse logs</a></h2>

<p>
In their raw form access logs are technically human-readable, but they are
a lot. I found it really useful to do a little parsing to pull out the bits
I'm interested in.
(I'm working with the default nginx log format, so adjust this
according to your own.)
</p>

<p>
I <a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/log_reader.py">wrote a script</a>
to take advantage of the repeatable structure of these logs
to dissect them into their parts. It uses tricks like splitting the log
based on brackets and spaces. It productes a pandas dataframe with
columns containing the IP address, requested URI, HTTP status code,
and every component of the date and time.
</p>

<p>
If you use exactly the same setup and log format as I do you might be
able to get away with using this code right out of the box. More likely
you'll have to (or want to) adjust it a bit for your own circumstances.
Make it your own!
</p>

<p>
This better organized version of the log can be used to generate a list
of pages that were queried, a list of IP addresses that visited the site,
or even just a chronological list of every access attempt.
</p>

<p>
Here's a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/pages.py">pages.py</a>
script that shows how many times pages were hit in a given day, for example
</p>

<p>
<figure>
  <img title="Count and page names" alt="Sample output from pages.py
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/pages_outputs.png">
  <figcaption>Count and page names</figcaption>
</figure>
</p>

<p>
Here's a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/ips.py "Count and IP address"">ips.py</a>
script that shows how many times a particular IP address visited that day
</p>

<p>
<img alt="Sample output from ips.py
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/ips_outputs.png">
</p>

<p>
Here's a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/history.py">history.py</a>
script that just repeats the access logs directly, but in a stripped down format that's easier to read.
</p>

<p>
<figure>
  <img title="Time, HTTP status, IP address, and page name" alt="Sample output from history.py
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/history_outputs.png">
  <figcaption>Time, HTTP status, IP address, and page name</figcaption>
</figure>
</p>

<h2><a id="Browsing-the-logs"></a><a href="#Browsing-the-logs">Browsing the logs</a></h2>

<p>
At first glance these logs are just an unbroken wall of text, but after
spending a couple of minutes with them, oddities emerge.
</p>

<p>
Looking at the IP address access count, why is there one address that accessed
the website 633 times? That's more than four times the next most frequent.
What was happening there? Surely that can't be legit, can it?
</p>

<p>
Looking at the access history, why was someone trying to find a
2016 New York Times article on this website? (The funny characters that come
before are a utf-8 encoding of a unicode right double quotation mark.)
That looks like someone was really flailing.
</p>

<p>
Looking at the pages viewed, once you get past the stylesheets,
javascripts, feed.xml., and robots.txt, there is the top-level blog,
a popular post about transformers which I put a ton of effort into, and
then <code>ssh_at_home</code>, a hastily-written set of notes about setting
up an ssh server for personal use. Why is that one so regularly visited?
</p>

<p>
The longer you look, the more questions arise. Some of the most
interesting patterns come from people doing mischief.
</p>

<h2><a id="Bad-behavior-#1:-Scanning-for-secrets"></a><a href="#Bad-behavior-#1:-Scanning-for-secrets">Bad behavior #1: Scanning for secrets</a></h2>

<p>
<figure>
  <img title="Looking for juicy passwords and tokens" alt="Access log history showing scanning for secrets
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/scanning_secrets.png">
  <figcaption>Looking for juicy passwords and tokens</figcaption>
</figure>
</p>

<p>
Here, a single IP address quickly tries to access a handful of variations
on a <code>.env</code> file, which is a common place to store access tokens
and other credentials. There is no good reason to be doing this, unless
you are doing it to yourself, proactively scanning for vulnerabilities
in order to fix them.
</p>

<p>
In my judgment this is a one-strike-and-you're out behavior. This IP
address will get added to my block list.
Luckily we already blocked access to all dot-files in the nginx server block
during initial setup. That's why these are resulting in a 403 status code
(access denied) rather than a 404 (not found). But if someone is doing this
they may be looking for other ways into your system. Why not just block
them at the firewall?
</p>

<h2><a id="Bad-behavior-#2:-Fishing-for-files"></a><a href="#Bad-behavior-#2:-Fishing-for-files">Bad behavior #2: Fishing for files</a></h2>

<p>
<figure>
  <img title="Someone really wants my website to be written in php" alt="Access log history showing fishing for php files
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/fishing_files.png">
  <figcaption>Someone really wants my website to be written in php</figcaption>
</figure>
</p>

<p>
In this segment, a single IP address is trying to find a
php file, and they appear to be randomly casting about. These files
don't exist on my webserver and never have. This requestor appears to
be shooting in the dark for files that they do not have a link to.
I don't know why, but I don't like it. I have all
php files blocked in my server block, but still this behavior shows someone
doing something other than browsing my website, which leads me not
to trust them. For me, this is a blockable offense.
</p>

<p>
Other file fishing I see often is for WordPress-related files.
</p>

<p>
<figure>
  <img title="Someone really wants my website to be written in WordPress" alt="Access log history showing fishing for WordPress files
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/fishing_wordpress.png">
  <figcaption>Someone really wants my website to be written in WordPress</figcaption>
</figure>
</p>

<p>
I also get requests trying to access links from my pages, as if they were
hosted on my server, such as
<code>/%20%20%20%20%20%20%20%20%20%20%20%20https:/en.wikipedia.org/wiki/Convolution</code>.
The <code>%20</code> is URL encoding for a space character. I also don't know why
there are 12 spaces in front of the URL. It looks like sloppy automated
parsing of a bot accessing links that were extracted from my html files.
I don't know what benign purpose this could serve. I'm content to
block these as well.
</p>

<p>
With the history browser there is a <code>--status</code> argument that lets you
quickly see just the 404's and 403's and 400's (bad request error)
and helps the file fishing to pop out.
</p>

<p>
<pre>
uv run history.py --status 404
</pre>
</p>

<h2><a id="Bad-behavior-#3:-Rapid-fire-requests"></a><a href="#Bad-behavior-#3:-Rapid-fire-requests">Bad behavior #3: Rapid-fire requests</a></h2>

<p>
<figure>
  <img title="More than ten requests per second is a lot." alt="Access log history showing two dozen requests in the span of two seconds
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/too_fast_requests.png">
  <figcaption>More than ten requests per second is a lot.</figcaption>
</figure>
</p>

<p>
Sometimes an IP address will make a lot of requests in quick succession.
Most of the time this is a mild annoyance, but when taken to an extreme
it floods the server, preventing any other requests from getting through.
It denies everyone else service, earning the name Denial of Service (DOS),
and whether done maliciously or through negligence, the effect on your
website is the same.
</p>

<p>
A lot of back-to-back requests is almost always a hallmark of automated
scraping. It's a personal judgment call, how much you want to support this.
The intended audience for my website is individuals, rather than
AI-training companies or even search engines, so I'm comfortable
making life uncomfortable for bulk-scrapers. There are two ways to do this.
</p>

<p>
The first is to set up rate limiting. One rate limiting mechanism is a
polite request in the <code>robots.txt</code> file to limit requests to, say,
one every 5 seconds.
</p>

<p>
<pre>
User-agent: *
Crawl-delay: 5
</pre>
</p>

<p>
Unfortunately, manners are in short supply on the internets, and most
crawlers and scrapers, including Google, ignores this directive. We can
resort to more draconian measures and use nginx to implement per-IP rate
limiting on the webserver.
</p>

<p>
This is done with a modification to the server block, as
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/commit/ad139aa48eb2d173d9dc9114759e48c485375b9e/server_blocks/brandonrohrer.com#L4">here</a>
and
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/commit/ad139aa48eb2d173d9dc9114759e48c485375b9e/server_blocks/brandonrohrer.com#L15">here</a>
</p>

<p>
<pre>limit_req_zone $binary_remote_addr zone=one:1m rate=1r/s;<br>
server {
    limit_req zone=one burst=10 nodelay;
    limit_req_status 429;
}
</pre>
</p>

<p>
These lines create a "zone", a 1MB history of IP addresses
that have made requests. On average, they should be making no more than
1 request per second. It allows for "bursts" of 10 additional requests,
serving them immediately with no delay, but anything in excess of that which
violates the rate limit will receive a HTTP status code of 429
(too many requests).
</p>

<p>
There are a lot of possible variations to this, but this is the basic
pattern. For a deep dive, check out
<a href="https://nginx.org/en/docs/http/ngx_http_limit_req_module.html">the nginx docs</a>.
</p>

<p>
And of course it's always possible to block IP addresses who try to pull this.
</p>

<h2><a id="Bad-behavior-#4:-Trying-to-hide-rapid-fire-requests"></a><a href="#Bad-behavior-#4:-Trying-to-hide-rapid-fire-requests">Bad behavior #4: Trying to hide rapid-fire requests</a></h2>

<p>
<figure>
  <img title="You're not fooling anyone." alt="Access log history showing three dozen coordinated requests in one second
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/botnet.png">
  <figcaption>You're not fooling anyone.</figcaption>
</figure>
</p>

<p>
Given how easy it was to rate limit a single IP address it should come as no
surprise that enterprising webscrapers have found a cheat. If they split their
requests across a whole bunch of IP addresses, then rate limiting on a
single address doesn't slow them down.
</p>

<p>
This even hides behing a respectable-sounding techy name: <a href="https://medium.com/@datajournal/best-rotating-proxies-tested-303539da1e2a">rotating proxies</a>.
But underneath it's just a way to get around website admins' express desire
that these jerks <em>not</em> do this thing.
</p>

<p>
Detecting this is trickier. In the example above, it's clear to a human
eye, that the requests are part of a coordinated scraping operation.
They all occur within one second. They are all requesting a .png
within the same directory. The IP addresses, while containing just
a few repeats, fall into a handful of clusters. But writing rules
for automating this is hard. Every rule you can come up with will
probably miss some coordinated scraping, or deny some legitimate traffic,
or both. Even machine learning methods, which can take multiple factors
into account, may not be able to do this cleanly.
</p>

<p>
Of course dangling a tricky problem like this in front of nerd is like waving
a red cape in front of an angry bull. I'll probably come back to deeper
treatment of it later. For now the only strategy I can recommend is manually
blocking every single IP address involved.
</p>

<h2><a id="Bad-behavior-#5:-Attempting-unsupported-actions"></a><a href="#Bad-behavior-#5:-Attempting-unsupported-actions">Bad behavior #5: Attempting unsupported actions</a></h2>

<p>
<figure>
  <img title="Someone with PROPFIND is not to be deterred." alt="Some HTTP request actions other than GET
" src="https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/hosting/illegal_actions.png">
  <figcaption>Someone with PROPFIND is not to be deterred.</figcaption>
</figure>
</p>

<p>
There are just a handful of things you can do over HTTP. <code>GET</code> and <code>POST</code> are
the most common, and <code>HEAD</code> comes up sometimes (get info about a page
without downloading it) but there are others that almost never come up in
the normal course of events. <code>PROPFIND</code> is a way to gather information about
all the available files on a server. It's easy to imagine how convenient that
would be for a scraper. <code>CONNECT</code> sets up an open pipe for data to flow to
and from a server. Nothing I would want to enable for a client I don't know
and trust.
</p>

<p>
For my little static website in particular, the only valid actions are
<code>GET</code> and <code>HEAD</code>. There is nothing to <code>PUT</code> or <code>POST</code> and everything
else I have no intention of allowing. I've never offered these actions
for any purpose, and it's very likely that any requests that contain them
are trying to get access to data and functionality they shouldn't have.
Block-worthy behavior in my book.
</p>

<h2><a id="Blocking-revisited"></a><a href="#Blocking-revisited">Blocking revisited</a></h2>

<p>
From the list above, there are a lot of offenses that can get an IP
address blocked, and a lot of IP addresses that commit them.
It would be possible to manually update the firewall rules for each one
at the command line by running something like this
</p>

<p>
<pre>
sudo ufw insert 1 deny from 101.101.101.101
</pre>
</p>

<p>
for every blocked address, but after my blocklist passed several dozen addresses
it started to feel tedious.
Updating the list became difficult after the list passed 100 addresses.
When adding new addresses manually I didn't want to duplicate the ones
that were already there, so I sorted them and added new ones into the
list where they belong. Duplicates were readily apparent. But after the
list grew to fill several screens lengths, updating became slow.
</p>

<p>
To streamline, I created a
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/blocklist_additions.txt">blocklist_additions.txt</a>
with every IP address I wanted to disallow. Then a small Python script
<a href="https://codeberg.org/brohrer/webserver-toolbox/src/branch/main/update_firewall.py">update_firewall.py</a>
was all I needed to automatically run the list.
</p>

<p>
<pre>
sudo python3 update_firewall.py
</pre>
</p>

<p>
This lets me browse through the logs and manually add problematic
IP addresses to the file <code>blocklist_additions.txt</code> on the server.
Then I can update the firewall with the latest changes.
</p>

<p>
Some of these behavior violations should be straightforward to detect
programmatically. Automating firewall updates will be an adventure
for another day.
</p>

<p>
October 6, 2025
</p>
    ]]></description>
  </item>

  </channel>
</rss>
