<rss version="2.0">
<channel>
<title>Brandon Rohrer</title>
<link>https://www.brandonrohrer.com</link>
<description>Brandon Rohrer's blog</description>


  <item>
    <title>
    Graffiti Wall: A commons drawing app
    </title>
    <link>
    https://graffitiwall.nexus/graffiti_wall.html
    </link>
    <pubDate>
    Mon, 09 Feb 2026 08:36:00 EDT
    </pubDate>
    <guid>
    https://graffitiwall.nexus/graffiti_wall.html
    </guid>
    <description><![CDATA[

<p>
The first step in building a small-scale LLM, an
<a href="https://brandonrohrer.com/alms.html">Artisanal Language Model</a>, is to
create a vocabulary of tokens. This converts a long string of text into
a somewhat shorter list of integers. We don't think about this step
often because it happens behind the scenes of LLM creation, but
building a model from scratch gives us the rare opportunity to look
more closely at tokenization.
</p>

<p>
The tokenizer is the first heavy duty processing that input data gets
exposed to when it churns through an LLM. The tokenizer takes the text
or audio or image and breaks it down into a sequence of bite-sized pieces
called tokens.
</p>

<p>
In theory, an LLM could operate on any discrete piece
of information, even fine-grained ones like characters, pixels, and bytes,
but in practice these are too small to carry much information individually
and working with them directly puts a greater burden on the model
to try to combine them and make sense of them.
</p>

<p>
To give the models a leg up, the tiny pieces of input get pre-combined
into more useful bits called tokens. For instance, when processing a
string of English text like <code>How much wood could a woodchuck chuck?</code>, it
might be tokenized into <code>How much</code>, <code>wood</code>, <code>could</code>, <code>a wood</code>, <code>chuck</code>,
<code>chuck</code>, <code>?</code>. Tokens don't fall strictly on word boundaries and can contain
more than one word. Handling 7 tokens instead of 38 characters makes life
a lot easier for the stages that follow. Internally, each of these word
pieces gets assigned a number for efficient handling, so this sentence
ends up looking like <code>[6387, 73, 593, 5365, 837, 837, 24]</code>.
</p>

<p>
Tokenization is an example of automated feature engineering,
also known as feature learning or
feature discovery. It is understood among machine learning practitioners
that once you get a good set of features, you're 80% of the way to
solving your problem. It is an important step to get right, and has a huge
impact on the quality of results.
</p>

<p>
An advantage to an Artisanal Language Model with a curated data set is
that it gives an opportunity to create a model-specific tokenizer.
The representation of input data can be tailored to the needs of your
specific problem, instead of having to create something that could
potentially cover every conceivable question someone could ask and
task they could propose.
</p>

<h2><a id="Vocabulary"></a><a href="#Vocabulary">Vocabulary</a></h2>

<p>
The collection of tokens that a tokenizer works with is called its
vocabulary. The tokenizers used by popular LLMs have vocabularies of
100,000 or more different tokens.
</p>

<p>
The size of the vocabluary is an important design parameter
that has a big effect on how big the model needs to be and how well it can
perform. A larger vocabulary means that word chunks and other pieces
of input can be bigger, saving the model the trouble of splicing them
together. It means that a given sequence of tokens can represent
a longer history of inputs. But it also means that the model has
a larger number of relationships to consider. Any given input token
might be predictive of any given output token, making the size of the
that space <em>O(N^2)</em>&mdash;something that grows as the <em>square</em> of the the
number of tokens. And when you add in the fact that it's not just
the current token, but the whole context history of tokens, that complexity
goes up further. (Attention is the magic trick that keeps it from
exploding catastrophically. More on that later.)
</p>

<p>
The flip side of this relationship is that reducing a vocabulary to 10%
of its original size means that the demands on
the model are reduced by far more than
that, down to something like 1% or 0.1%. The amount of training data needed,
the computation required, the inference time, all get reduced. The smaller
the vocabulary, the less time and expense to train and use a model.
</p>

<p>
Having an ALM focused on a single task with a curated data set makes
this possible. The more targeted the data set, the fewer the tokens
needed to represent it well.
</p>

<p>
All the numbers I mention above are very hand wavy because LLMs are so very
expensive to train and evaluate. I'm not aware of any research published
about the exact nature of the relationship between
vocabulary size, number of model
parameters, and model performance. A smaller ALM that is feasible to
re-train a number of times make this investigation approachable.
It means that the tokenizer for a given ALM can be trial-and-error optimized
for its intended usage.
</p>

<h2><a id="Example-use-case:-Proofreading-English-prose"></a><a href="#Example-use-case:-Proofreading-English-prose">Example use case: Proofreading English prose</a></h2>

<p>
Since the ALM is task-specific, the tokenizer can be chosen to be
match the task. So in order do choose the tokenizer it is first
necessary to define the task.
</p>

<p>
To choose an arbitrary example, the task is to proofread a
manuscript draft and highlight words or phrases
that might not be what the author intended. The manuscript will
be in English, in the general vein of classic science fiction and fantasy.
</p>

<p>
The tokenizer will need some data to train on. The exact set of training
texts don't matter too much because it will always be possible to
retrain the tokenizer later.
<a href="https://www.gutenberg.org">Project Gutenberg</a> is a collection of Public
Domain books and a great place to collect a starter pack of training texts.
I pulled ten for training
</p>

<ol>
<li> <em>Dracula</em> by Bram Stoker</li>
<li> <em>Hound of the Baskervilles</em> by Arthur Conan Doyle</li>
<li> <em>The Legend of Sleepy Hollow</em> by by Washington Irving</li>
<li> <em>The Lost World</em> by Arthur Conan Doyle</li>
<li> <em>Ozma of Oz</em> by L. Frank Baum				</li>
<li> <em>The Picture of Dorian Gray</em> by Oscar Wilde</li>
<li> <em>Seven Dials Mystery</em> by Agatha Christie</li>
<li> <em>The Time Machine</em> by H. G. Wells</li>
<li> <em>Twenty Thousand Leagues Under the Sea</em> by Jules Verne</li>
<li> <em>War of the Worlds</em> by H. G. Wells</li>
</ol>

<p>
and set aside two for evaluation.
</p>

<ol>
<li> <em>Alice's Adventures in Wonderland</em> by Lewis Carroll</li>
<li> <em>Frankenstein</em> by Mary Wollstonecraft Shelley</li>
</ol>

<h2><a id="Choosing-a-tokenizer"></a><a href="#Choosing-a-tokenizer">Choosing a tokenizer</a></h2>

<p>
There are a few common flavors of tokenizer,
</p>

<ul>
<li> <strong>Byte-Pair Encoding (BPE)</strong> (byte-level byte-pair encoding is a variation)</li>
<li> <strong>WordPiece</strong></li>
<li> <strong>Unigram</strong></li>
<li> <strong><a href="https://huggingface.co/papers/1808.06226">SentencePiece</a></strong></li>
</ul>

<p>
These are described well in
<a href="https://huggingface.co/docs/transformers/tokenizer_summary">this summary by Huggingface</a>
</p>

<p>
WordPiece splits on spaces, so it's a good fit for some languages
(including English), but not others (such as Chinese).
Default implementations of BPE assume space-delimitation too.
Unigram is similar to BPE except that
1) it starts with a huge set of token candidates and eliminates them,
rather than starting with non and adding them and 2) it chooses tokens
based on highest probability of co-occurrence, rather than raw frequency.
SentencePiece handles sentences as a whole, without assuming spaces
between words. It works in conjunction with BPE or Unigram to generate
tokens.
</p>

<p>
After looking through all the options, I decided to start with SentencePiece.
It is a good fit for the use case, and comes ready to use in
<a href="https://github.com/google/sentencepiece">a repository</a> from Google, also
available through PyPi.
</p>

<h2><a id="Training-the-tokenizer"></a><a href="#Training-the-tokenizer">Training the tokenizer</a></h2>

<p>
Training a SentencePiece tokenizer is straightforward thanks to the public
repo and <a href="https://github.com/google/sentencepiece/blob/master/python/README.md">the Python wrapper</a>
that can be installed with pip or <code>uv add sentencepiece</code>. It has a lot
of good defaults, so you can train a good tokenizer with a short call:
</p>

<p>
<pre>
import sentencepiece as spm
spm.SentencePieceTrainer.train(input="training_file.txt", model_prefix="m")
</pre>
</p>

<p>
And it is shockingly fast. I was pretty sure mine had errored out because it
finished in just a couple of seconds, but that's just how long it took.
</p>

<p>
A short demo shows that it's working.
</p>

<p>
<pre>
sp = spm.SentencePieceProcessor()
sp.load("m.model")
test_sentence = "This is a test of the sentencepiece tokenizer."
test_pieces = sp.encode_as_pieces(test_sentence)
print(test_pieces)
test_ids = sp.encode_as_ids(test_sentence)
print(test_ids)<br>
print(sp.decode_pieces(test_pieces))
print(sp.decode_ids(test_ids))
</pre>
</p>

<p>
There are lots of options to tweak how the tokenizer works. At this point
there isn't a great way to say which is better. That can only happen
once it's connected to a language model and performing a task. But here
I'll note some variations and hyperparameters that will be worth
experimenting with.
There's <a href="https://github.com/google/sentencepiece/blob/master/doc/options.md">an exhaustive list here</a>
for reference. Here are some of the arguments that appear useful for
building a proofreading tool.
</p>

<h4><a id="<code>input</code>"></a><a href="#<code>input</code>"><code>input</code></a></h4>

<p>
This is the biggest one. It specifies the training data. You can include
multiple files in a comma-separated list like so
<pre>
input="training_file_1.txt,training_file_2.txt,training_file_3.txt"
</pre>
</p>

<p>
Choosing the inputs is by far the biggest factor in customizing the tokenizer.
</p>

<h4><a id="<code>model_prefix</code>"></a><a href="#<code>model_prefix</code>"><code>model_prefix</code></a></h4>

<p>
This is just the name of the model files that get generated.
For <code>model_prefix="m"</code>, the files created are <code>m.model</code> and <code>m.vocab</code>.
</p>

<h4><a id="<code>vocab_size</code>"></a><a href="#<code>vocab_size</code>"><code>vocab_size</code></a></h4>

<p>
The total number of tokens to create, e.g. <code>vocab_size=20000</code>. Defaults to
8000.
</p>

<h4><a id="<code>model_type</code>"></a><a href="#<code>model_type</code>"><code>model_type</code></a></h4>

<p>
Can be <code>unigram</code> (default), <code>bpe</code>, <code>char</code>, or <code>word</code>.
The input sentence must be pretokenized when using word type.
</p>

<h4><a id="<code>split_by_whitespace</code>"></a><a href="#<code>split_by_whitespace</code>"><code>split_by_whitespace</code></a></h4>

<p>
If <code>True</code> (default), this makes sure to split all words at space
boundaries and won't create multi-word tokens.
</p>

<h4><a id="<code>normalization_rule_name</code>"></a><a href="#<code>normalization_rule_name</code>"><code>normalization_rule_name</code></a></h4>

<p>
There are a few ways to pre-process the data to clean it up and make it
more uniform.
</p>

<ul>
<li> <code>identity</code>: no normalization</li>
<li> <code>nfkc</code>: NFKC normalization, which makes sure Unicode is represented in a consistent way.</li>
<li> <code>nmt_nfkc</code> (default): NFKC normalization with some additional
normalization around spaces</li>
<li> <code>nfkc_cf</code>: <code>nfkc</code> + Unicode case folding (mostly lower casing)</li>
<li> <code>nmt_nfkc_cf</code>: <code>nmt_nfkc</code> + Unicode case folding </li>
</ul>

<p>
For proofreading, the <code>identity</code> is the best option, because I don't want
any irregularities to get swept under the rug.
</p>

<h4><a id="<code>remove_extra_whitespaces</code>"></a><a href="#<code>remove_extra_whitespaces</code>"><code>remove_extra_whitespaces</code></a></h4>

<p>
If <code>True</code> (default), removes leading, trailing, and duplicate internal whitespace.
For proofreading I'll set this
to False so that any undesirable spacing can be detected.
</p>

<p>
<br>
</p>

<p>
These options can all be expressed as Python arguments or as a command
line style string. For example, this
</p>

<p>
<pre>
spm.SentencePieceTrainer.train(
    input="inputs.txt"
    model_prefix="m",
    model_type="unigram,
    vocab_size=10000,
)
</pre>
</p>

<p>
or this
</p>

<p>
<pre>
spm.SentencePieceTrainer.train(
    "--input=inputs.txt --model_prefix=m --model_type=unigram --vocab_size=10000"
)
</pre>
</p>

<p>
As far as I can tell they produce identical results, meaning that whoever
wrote the Python wrapper did a good job of convering the argument list,
so you can choose whichever approach speaks to you.
</p>

<p>
These pieces are all assembled into
<a href="https://codeberg.org/brohrer/alms/src/branch/main/src/proofread_eng_scifi/tokenizer_example.py">some example code</a>
for reference that you can dowload and play with.
</p>

<p>
The cool part about this is that that even though tokenization is a
preprocessing step, we've already been able to customize it to our use case.
This is a luxury of ALMs that LLMs cannot afford; they are stuck trying
to cover such a broad set of use cases that they can't optimize for a
single one. This raises the intriguing possibility that ALMs might not
only be able to match the performance of LLMs on their chosen task,
but may even be able to exceed it.
</p>


    ]]></description>
  </item>

  <item>
    <title>
    Artisanal Language Models
    </title>
    <link>
    https://brandonrohrer.com/alms.html
    </link>
    <pubDate>
    Wed, 18 Feb 2026 08:36:00 EDT
    </pubDate>
    <guid>
    https://brandonrohrer.com/alms.html
    </guid>
    <description><![CDATA[


<p>
Large Language Models are everywhere, and in February 2026
it's hard to imagine what the machine learning world would look like
without them. But it's fun to imagine some alternatives.
</p>

<h2><a id="What-if-an-LLM-were-trained-to-complete-a-specific-task?"></a><a href="#What-if-an-LLM-were-trained-to-complete-a-specific-task?">What if an LLM were trained to complete a specific task?</a></h2>

<p>
LLMs are general purpose tools, trained to be able to do many
different things. As a result they are vast. And they are better at
doing some things than others.
</p>

<p>
If an LLM were instead focused on a particular task, trained to perform it
as well as possible and ignore everything else, then it would have
a better chance at being excellent at that thing, rather than just
being OK at everything.
</p>

<p>
The structure around the model, the preprocessing of the input data and
postprocessing out the output data, could all be specially built to
give the best results on a single task. Whether for document retrieval
or spell checking or autocomplete or interactive chat, whether for text or
for photos or for music, an Artisanal Language Model could be focused
on doing one thing and doing it well.
</p>

<h2><a id="What-if-we-could-train-an-LLM-on-a-data-set-we-collected-ourselves?"></a><a href="#What-if-we-could-train-an-LLM-on-a-data-set-we-collected-ourselves?">What if we could train an LLM on a data set we collected ourselves?</a></h2>

<p>
LLMs are trained on practically the whole internet.
</p>

<p>
The downside on such a broad swath of training data is that LLMs are also
trained on every stupid thing someone decided to post in a drunken rant.
ML engineers try to correct for this with post-processing filter steps
and prescriptive prompts, but there's no way to completely eliminate
idiocy from the model once it's been trained in.
</p>

<p>
With a set of curated training
data, every inclusion is a deliberate decision, and garbage-in-garbage-out
becomes less of a problem.
Curated training data would also avoid the many legal and ethical viiolations
called out in the training sets of popular LLMs, including copyright
violation, license violation, nonconsensual sharing of personal images,
and inclusion of CSAM.
</p>

<p>
Another advantage of an ALM focused on a specific task is that it can
limit its training data to what is relevant to the problem at hand.
If building a Javascript code completion model, we wouldn't need to include
large amounts of prose every language of the world, or even code in other
computer languages. We wouldn't need to include catalogs of audio recordings
or millions of images. We could limit the training data to Javascript, making
it orders of magnitude smaller.
</p>

<p>
With smaller training data sets, training itself becomes more feasible.
For LLMs we can count on one hand the companies large enough to train their
own general purpose models from scratch. The investment in data engineering
and computation is huge. But for a training data set that is 0.01% of
the size, the training time and cost come down within the reach of
many more organizations, researchers, and hobbyists.
</p>

<h2><a id="What-if-an-LLM-could-be-trained-on-CPUs?"></a><a href="#What-if-an-LLM-could-be-trained-on-CPUs?">What if an LLM could be trained on CPUs?</a></h2>

<p>
Training a modern LLM requires centuries of GPU time. This gets spread across
many thousands of GPUs so that it completes in a reasonable amount of time,
but it remains massive. And it results in huge hardware and power bills.
</p>

<p>
If a much smaller model were capable of being trained on CPU only, even if
it required many of them, it would remove a huge barrier to language model
training. The rate of experimentation, innovation, diversification, and
specialization would increase tremendously.
</p>

<h2><a id="What-if-an-LLM-could-run-on-your-laptop?"></a><a href="#What-if-an-LLM-could-run-on-your-laptop?">What if an LLM could run on your laptop?</a></h2>

<p>
The current generation of LLMs require clusters of GPUs for inference.
Even after they are
trained, they require too much computation and hardware to sit comfortably
in anything but a data center.
</p>

<p>
If there were language models small enough to run on a laptop and nimble
enough to return inference results quickly on laptop hardware, their
deployment could be made more robust. No network latency, no reliance on
connectivity, no dependence on external services' uptime. The reliability
engineering would become considerably more straightforward and the
expectations of availability could be raised.
</p>

<p>
This also opens the way for proprietary models and applications requiring
high security. The ability to run isolated from an external network
opens up new domains.
</p>

<h2><a id="What-if-an-LLM-could-continue-to-learn-as-you-use-it?"></a><a href="#What-if-an-LLM-could-continue-to-learn-as-you-use-it?">What if an LLM could continue to learn as you use it?</a></h2>

<p>
Current LLMs are too unwieldy to be updated on the fly. While there are
ways to refine them, such as fine tuning or reinforcement learning
from human feedback, these are either tweaks to a small part of the model
or post-processing steps. They aren't capable of updating the model as a whole.
</p>

<p>
If it were possible to update the model based on every interaction, every
new bit of input data and user response, that would make it possible for
the model to get better over time in a meaningful way. And most importantly,
it would continue to adapt to the specific users, tasks, and input data
it was exposed to. It would learn the parameters of its job under
the continuous mentorship of its human users.
</p>

<p>
<br>
<br>
What if we had Artisanal Language Models?
</p>

<p>
I hope to find out.
</p>

    ]]></description>
  </item>

  <item>
    <title>
    Graffiti Wall: A commons drawing app
    </title>
    <link>
    https://graffitiwall.nexus/graffiti_wall.html
    </link>
    <pubDate>
    Mon, 09 Feb 2026 08:36:00 EDT
    </pubDate>
    <guid>
    https://graffitiwall.nexus/graffiti_wall.html
    </guid>
    <description><![CDATA[
<p>
I made public space where anyone and everyone can go and scribble anonymously.
Probably a mistake, but it taught me a lot.
</p>
<p>
<a href="https://graffitiwall.nexus/graffiti_wall.html">
Graffiti Wall</a> (
<a href="https://codeberg.org/brohrer/graffitiwall.nexus">
  frontend code</a>,
<a href="https://codeberg.org/brohrer/graffitiwall-server">
  backend code</a>)
</p>
<p>
It's the latest step on my journey, stitching together everything
I've been learning about
self-hosting, deploying a WSGI server, and backing it with a
Postgres database.
</p>
<p>
Please enjoy it!
</p>

    ]]></description>
  </item>


  <item>
    <title>
    An addition app
    </title>
    <link>
    https://graffitiwall.nexus//graffiti_adder.html
    </link>
    <pubDate>
    Sun, 28 Dec 2025 08:36:00 EDT
    </pubDate>
    <guid>
    https://graffitiwall.nexus//graffiti_adder.html
    </guid>
    <description><![CDATA[
<p>
I made a little
<a href="https://graffitiwall.nexus/graffiti_adder.html">
Addition App</a> (
<a href="https://codeberg.org/brohrer/graffitiwall.nexus">
  frontend code</a>,
<a href="https://codeberg.org/brohrer/graffitiwall-server">
  backend code</a>)
to stitch together everything I've been learning about
self-hosting, deploying a WSGI server, and backing it with a
Postgres database.
</p>
<p>
Please enjoy it!
</p>

    ]]></description>
  </item>


  </channel>
</rss>
